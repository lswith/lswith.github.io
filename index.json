[{"content":"Introduction In the second part of this blog series inspired by Team Topologies, I explore how teams communicate and interact with one another. Effective team communication is a cornerstone of organizational success, but it’s often easier said than done. This post dives into key interaction modes and practices that can help teams strike the right balance between autonomy and collaboration, ultimately improving flow and reducing friction.\n1. The Challenge of Team Communication\nA team may have been told it is autonomous and self-organizing, but team members often find they have to interact with many other teams to complete their work. This creates a tension between autonomy and dependency. If not managed carefully, it can lead to inefficiencies, misaligned responsibilities, and frustration.\nThe key challenge, as Team Topologies highlights, is to avoid a situation where all teams need to communicate with all other teams to achieve their goals. Similar to how a jazz band carefully coordinates its music, organizations must curate team communication to ensure clarity, focus, and purpose.\n2. Modes of Team Interaction\nTeam Topologies outlines three primary modes of team interaction:\nCollaboration: Teams work closely together to achieve a shared objective. This mode is suited to situations where teams need to pool expertise and responsibilities for a specific goal. However, collaboration often blurs responsibility boundaries and increases communication overhead. While this can reduce apparent effectiveness at the team level, it fosters rapid discovery and innovation at the ensemble level.\nExample: Two teams with distinct expertise and responsibilities collaborate on a shared feature, temporarily functioning as a single unit. Collaboration can also benefit from temporary shifts in interaction modes, which refresh team dynamics and increase empathy between teams.\nX-as-a-Service: One team provides a code library, API, or platform as a service to another team, with minimal collaboration required. This mode works well when the service is well-implemented and fit for purpose, enabling consuming teams to use it without much effort. However, if the consuming team spends excessive time trying to use the service, this signals a problem with the service boundary or implementation.\nExample: A platform team provides a CI/CD pipeline as a service, allowing development teams to deploy their code without needing to know the underlying infrastructure. Proper service boundaries ensure low-friction interactions and limited communication needs.\nFacilitating: One team helps another team clear impediments or adopt new practices. Facilitating teams typically work across multiple teams, identifying cross-team problems and coaching teams to improve capabilities.\nExample: An enabling team helps a stream-aligned team adopt test-first development practices.\n3. Tailoring Interaction Modes\nInteraction modes should not be static but instead become habitual patterns tailored to the needs of the teams involved. A single team might use two different interaction modes for two different teams it works with. This flexibility allows organizations to address the unique challenges of each interaction, improving team engagement, clarity of purpose, and trust.\nBy deliberately curating interaction modes, teams can:\nReduce the frustration of misaligned expectations. Increase engagement and collaboration effectiveness. Maintain clarity of responsibilities and avoid blurred boundaries. If the current interaction mode has become stale or ineffective, temporarily changing it can provide fresh perspectives and foster greater empathy between teams.\n4. The Risks of Poor Communication Patterns\nWhen team interactions are not consciously designed, anti-patterns emerge:\nOver-collaboration: High communication overhead results from teams working too closely together without clear boundaries. This can reduce individual team effectiveness and lead to burnout.\nIll-defined Service Boundaries: X-as-a-Service interactions fail when the service is not well-implemented or lacks proper management practices. Teams become frustrated with services that require significant effort to use. As the book notes, if a stream-aligned team spends hours struggling to use a component, it’s a signal that something is amiss.\nLack of Facilitation: Without effective facilitating teams, cross-team problems persist, slowing down progress and innovation.\nTo address these issues, Team Topologies suggests using temporary but explicit collaboration modes between teams building software, supported by facilitating teams where necessary.\n5. Applying Interaction Modes in Practice\nWhen considering the relationship between any teams, a key decision is whether to collaborate, consume a service, or facilitate. Here are some principles to guide these decisions:\nTeams interacting in collaboration mode should expect high interaction and mutual respect, with shared responsibilities. Teams in X-as-a-Service mode should emphasize the user experience of the provided service, ensuring it is intuitive and fit for purpose. Teams in facilitating mode should expect to help and be helped, fostering an open-minded approach to improvement and shared learning. These modes should align with the reverse Conway maneuver—designing team structures and interactions to influence software architecture deliberately.\nFinal Reflections How teams communicate and interact has a profound impact on organizational success. By understanding and applying the interaction modes from Team Topologies, organizations can curate team dynamics to foster clarity, reduce frustration, and enhance overall flow. Whether through collaboration, X-as-a-Service, or facilitation, consciously designed team interactions help teams thrive and deliver exceptional results.\nStay tuned for the next post in this series, where I’ll delve further into the principles of team-first approaches and how they influence software design and delivery.\n","permalink":"https://lswith.io/posts/how-teams-interact/","summary":"\u003ch3 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eIn the second part of this blog series inspired by \u003ca href=\"https://teamtopologies.com/book\"\u003e\u003cem\u003eTeam Topologies\u003c/em\u003e\u003c/a\u003e, I explore how teams communicate and interact with one another. Effective team communication is a cornerstone of organizational success, but it’s often easier said than done. This post dives into key interaction modes and practices that can help teams strike the right balance between autonomy and collaboration, ultimately improving flow and reducing friction.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. The Challenge of Team Communication\u003c/strong\u003e\u003c/p\u003e","title":"How Teams Communicate and Interact"},{"content":"Understanding the \u0026ldquo;Constitution of Knowledge\u0026rdquo; by Jonathan Rauch Recently, I attended a Burning Man-style event that exposed me to a wide range of ideas and new ways of thinking about happiness and living life. One conversation stood out to me: how we integrate radical ideas into our society and decide which ideas to trust. This sparked my interest in Jonathan Rauch\u0026rsquo;s book, The Constitution of Knowledge: A Defense of Truth.\nIn his book, Rauch provides a compelling analysis of how societies can establish and sustain truth. Epistemic chaos, a central concern of the book, refers to a breakdown in shared understanding and agreement about what constitutes knowledge and truth. It occurs when misinformation and disinformation undermine trust in traditional sources of authority and expertise, leaving individuals and communities struggling to distinguish fact from fiction. This challenge is particularly pressing in an era of rampant misinformation and polarization.\nThis is a summary and reflection on the book, focusing on two central ideas: the concept of a reality-based community and the principles of the \u0026ldquo;Constitution of Knowledge.\u0026rdquo;\nWhat Is a Reality-Based Community? At its core, a reality-based community is a collective of individuals and institutions committed to discovering, verifying, and upholding objective truth. This community includes journalists, scientists, scholars, and others who adhere to norms and practices that prioritize evidence over ideology, critical inquiry over dogma, and collaboration over isolation.\nRauch argues that the reality-based community functions as a kind of societal immune system. It processes vast amounts of information, discards errors, and establishes a shared baseline of facts. This community is essential for any functioning democracy, as it allows for informed decision-making and rational debate. Without it, societies risk descending into epistemic anarchy, where the loudest voices or the most emotionally charged narratives overshadow objective truths.\nThe Tenets of the \u0026ldquo;Constitution of Knowledge\u0026rdquo; Rauch\u0026rsquo;s titular \u0026ldquo;Constitution of Knowledge\u0026rdquo; is not a formal document but a set of norms and institutions that collectively govern the pursuit of truth. Drawing parallels to the U.S. Constitution, he argues that just as political constitutions structure power to prevent tyranny, the Constitution of Knowledge structures epistemic practices to prevent error and bias. Its key principles include:\nThe Fallibilist Framework\nRecognize that no single individual or institution possesses the ultimate truth. All claims are subject to scrutiny, debate, and revision. Truth emerges not from certainty but from a process of constant testing, error correction, and refinement. Empirical Accountability\nClaims must be testable and grounded in observable evidence. Beliefs that cannot be verified or falsified have no place in the reality-based community. This principle demands rigorous peer review, open data, and transparency in methodology. Social Verification\nTruth is not a solitary endeavor. It is established through collaboration and consensus within a network of independent yet interdependent actors. Disagreements and challenges are not threats but essential components of the truth-seeking process. Rules, Not Rulers\nThe pursuit of knowledge must be governed by impartial rules, not by the whims of individuals or institutions. These rules include standards for evidence, logic, and debate that apply equally to everyone. Pluralism and Openness\nA diversity of perspectives and ideas strengthens the reality-based community. No viewpoint is dismissed a priori; all must compete in the marketplace of ideas. This openness fosters innovation and ensures that no single ideology monopolizes the pursuit of truth. Anti-Authoritarianism\nThe Constitution of Knowledge resists attempts to impose truth by authority, coercion, or manipulation. It stands as a bulwark against propaganda, censorship, and other forms of epistemic control. Conclusion In a world shaped by our cultural and community norms, it is vital to remain open to radically different ideas. These ideas might challenge our assumptions and stretch the boundaries of what we consider normal. However, openness does not mean blind acceptance. We must approach these ideas with understanding, while also rigorously examining and defending them using empirical evidence and social verification.\nThis requires humility—a recognition that no idea is immune to error. Every belief, no matter how strongly held, can be disproven or revised in light of new evidence. By adopting this mindset, we can strengthen our commitment to truth and contribute to a more resilient reality-based community.\n","permalink":"https://lswith.io/posts/constitution-of-knowledge/","summary":"\u003ch1 id=\"understanding-the-constitution-of-knowledge-by-jonathan-rauch\"\u003eUnderstanding the \u0026ldquo;Constitution of Knowledge\u0026rdquo; by Jonathan Rauch\u003c/h1\u003e\n\u003cp\u003eRecently, I attended a Burning Man-style event that exposed me to a wide range of ideas and new ways of thinking about happiness and living life. One conversation stood out to me: how we integrate radical ideas into our society and decide which ideas to trust. This sparked my interest in Jonathan Rauch\u0026rsquo;s book, The Constitution of Knowledge: A Defense of Truth.\u003c/p\u003e","title":"Understanding the Constitution of Knowledge"},{"content":"Introduction Recently, I started reading Team Topologies and quickly noticed that many of the key concepts in the book resonated deeply with my experiences at Dovetail. The most striking realization was the power of teams over individuals. This idea challenged some traditional notions of performance and productivity, highlighting that team structure and dynamics are far more important than individual brilliance. Reflecting on these concepts has helped me better understand some of the organizational issues we’ve encountered and provided a fresh lens for approaching potential solutions. This blog is the first in a series where I’ll explore the key learnings from Team Topologies and share how they relate to my own experiences.\n1. Teams, Not Individuals, Drive Success\nIn the bestselling book Team of Teams, retired US Army General Stanley McChrystal notes that the best-performing teams “accomplish remarkable feats not simply because of the individual qualifications of their members but because those members coalesce into a single organism.” This insight reflects the core principle that teams, not individuals, drive success in modern organizations.\nResearch from Google reinforces this point, finding that who is on the team matters less than the team’s dynamics. High-performing teams emphasise psychological safety, dependability, structure, clarity, and meaning—factors that are impossible to achieve through individual performance alone.\n2. The Impact of Team Size\nTeam size plays a significant role in maintaining effective team dynamics. Allowing teams to grow beyond the “magic” seven-to-nine size risks trust breakdowns, poor decisions, and reduced accountability. Instead, organizations should prioritise small, stable, and long-lived teams, as Allan Kelly suggests in his book Project Myopia. Stable teams help foster \u0026ldquo;continuity of care\u0026rdquo; for software systems, enabling higher quality, cleaner code, and improved system operability.\nIf we stress the team by giving it responsibility for part of the system that is beyond its cognitive load capacity, it ceases to act like a high-performing unit and starts to behave like a loosely associated group of individuals, each trying to accomplish their individual tasks without the space to consider if those are in the team’s best interest.\n3. Team Ownership and Accountability\nTeam ownership is another critical factor. When multiple teams can modify the same system or subsystem, accountability is blurred, and technical debt accumulates. However, a single team with full ownership of a subsystem can take control of short-term fixes and ensure they are addressed properly in the long term. Ownership enables better decision-making and continuous improvement, giving the team the autonomy to operate more effectively.\nAt the end of the day, technology teams need to invest in proven team practices like continuous delivery, test-first development, and a focus on software operability and releasability. Without them, all the effort invested in a team-first approach to work and flow will be greatly undermined or at least underachieved.\n4. Avoiding Anti-Patterns in Team Design\nPoor team design can lead to suboptimal outcomes. The first anti-pattern is ad hoc team design. This includes teams that have grown too large and been broken up as the communication overhead starts taking a toll, teams created to take care of all COTS software or all middleware, or a DBA team created after a software crash in production due to poor database handling. Of course, all of these situations should trigger some action, but without considering the broader context of the interrelationships between teams, what seems like a natural solution might slow down delivery and eat away at the autonomy of teams.\nThe other common anti-pattern is shuffling team members. This leads to extremely volatile teams assembled on a project basis and disassembled immediately afterward, perhaps leaving one or two engineers behind to handle the “hardening” and maintenance phases of the application(s). Such instability erodes trust, increases onboarding time, and decreases team cohesion.\nKey Takeaway: Teams, not individuals, are the foundation of effective software development. Small, long-lived teams with ownership and autonomy produce better outcomes and maintain healthier software systems.\nFinal Reflections Team Topologies offers a fresh perspective on team design. By focusing on team-first principles—like smaller, stable teams with clear ownership—companies can achieve faster, more predictable software delivery. The book’s insights into the importance of team structure, accountability, and avoiding anti-patterns provide a practical roadmap for organizations seeking to move from chaotic coordination to fast, intentional flow. By putting these principles into practice, organizations can build better teams—and better software—with fewer roadblocks along the way.\n","permalink":"https://lswith.io/posts/teams-over-individuals/","summary":"\u003ch3 id=\"introduction\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eRecently, I started reading \u003ca href=\"https://teamtopologies.com/book\"\u003e\u003cem\u003eTeam Topologies\u003c/em\u003e\u003c/a\u003e and quickly noticed that many of the key concepts in the book resonated deeply with my experiences at Dovetail. The most striking realization was the power of teams over individuals. This idea challenged some traditional notions of performance and productivity, highlighting that team structure and dynamics are far more important than individual brilliance. Reflecting on these concepts has helped me better understand some of the organizational issues we’ve encountered and provided a fresh lens for approaching potential solutions. This blog is the first in a series where I’ll explore the key learnings from \u003cem\u003eTeam Topologies\u003c/em\u003e and share how they relate to my own experiences.\u003c/p\u003e","title":"Why Teams Matter Over Individuals"},{"content":"Infrastructure as an Enabler, Not a Differentiator In the fast-moving world of technology, it’s easy to be drawn to the latest and greatest infrastructure solutions. But for most companies, infrastructure shouldn’t be seen as the centerpiece of innovation. Instead, it’s a critical enabler—a foundation that supports product development, rather than a driver of differentiation.\nInfrastructure Is Not Core to the Business Companies succeed by delivering unique, customer-centric experiences. The core of this value lies in product innovation—the features, interfaces, and improvements that users interact with directly. Infrastructure, while essential for enabling that innovation, exists in the background. It’s the scaffolding that holds everything up, not the product that defines the customer experience.\nThis perspective leads to an important realization: infrastructure’s role is to support innovation, not overshadow it. It’s the means to an end, not the end itself.\nInnovation Budgets Are for Customer Value With this view in mind, it’s crucial to protect the “innovation budget”—the time, energy, and resources dedicated to creating customer-facing value. If internal platform work or infrastructure projects consume too much of this budget, it reduces the capacity to innovate on the product.\nInfrastructure work should only draw from this budget if it enables new, customer-facing capabilities or significantly improves the user experience. Otherwise, it’s important to ask: is this investment in infrastructure taking away from what truly matters to the customer? Prioritization decisions should always be guided by the question, \u0026ldquo;How does this improve the customer experience?\u0026rdquo;\nEmbracing Boring Technology A natural extension of this philosophy is to embrace what’s often referred to as “boring technology.” While the phrase might sound unappealing, it’s a strategic and deliberate approach. Boring technology refers to well-established, widely supported tools that have a strong track record of reliability.\nWhy prioritize boring technology?\nReliability: Proven tools are less likely to fail unexpectedly. Supportability: Industry-standard tools have larger communities and better support. Maintainability: Fewer custom solutions mean less time spent on maintenance. This strategy means opting for stability and predictability over novelty and cutting-edge experimentation. When infrastructure is simple and stable, teams can focus on delivering product innovations rather than fighting fires in platform operations.\nLeverage Vendors and Open Source Another crucial part of the infrastructure strategy is to leverage vendor-supported solutions and open-source tools. Instead of building everything in-house, companies can adopt tools that are actively maintained and supported by external communities or providers.\nThis approach has several advantages:\nReduced Maintenance Burden: Vendor-managed services shift the maintenance and operational burden away from internal teams. Community Contributions: Open-source tools benefit from community-driven improvements, security patches, and new features. Faster Iteration: By using off-the-shelf solutions, teams can move faster and avoid reinventing the wheel. Rather than building custom systems from scratch, companies can tap into the shared expertise of the open-source community or rely on the dedicated support of a vendor. This enables teams to focus on core competencies and deliver better customer outcomes.\nReducing Complexity At the heart of this strategy is a commitment to simplicity. Every decision about platform tooling and infrastructure should aim to reduce cognitive load and operational complexity. The goal is to create a stable, low-maintenance foundation so that product teams can focus on building features and experiences that matter.\nReducing complexity might mean using a well-known managed database instead of self-hosting one, or adopting a platform-as-a-service (PaaS) instead of managing infrastructure directly. The less custom logic and infrastructure to maintain, the less overhead required to keep it running smoothly.\nWith simpler, well-understood technologies, teams can reduce \u0026ldquo;mental overhead\u0026rdquo; and spend more time focusing on customer-impacting work. This approach leads to faster development cycles, fewer surprises, and a more stable operational environment.\nConclusion Infrastructure should be seen as an enabler, not a differentiator. By adopting boring, reliable technologies, leaning on vendor and open-source solutions, and reducing complexity, companies can create a solid foundation for product innovation. This approach preserves the \u0026ldquo;innovation budget\u0026rdquo; for customer-facing features and experiences, rather than internal platform work.\nWhen infrastructure is predictable and well-maintained, teams can channel their energy into what matters most: delivering value to customers. It’s not about having the flashiest tech stack—it’s about having a stable platform that enables great products to flourish.\n","permalink":"https://lswith.io/posts/infra/","summary":"\u003ch1 id=\"infrastructure-as-an-enabler-not-a-differentiator\"\u003e\u003cstrong\u003eInfrastructure as an Enabler, Not a Differentiator\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003eIn the fast-moving world of technology, it’s easy to be drawn to the latest and greatest infrastructure solutions. But for most companies, infrastructure shouldn’t be seen as the centerpiece of innovation. Instead, it’s a critical enabler—a foundation that supports product development, rather than a driver of differentiation.\u003c/p\u003e\n\u003ch2 id=\"infrastructure-is-not-core-to-the-business\"\u003e\u003cstrong\u003eInfrastructure Is Not Core to the Business\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eCompanies succeed by delivering unique, customer-centric experiences. The core of this value lies in product innovation—the features, interfaces, and improvements that users interact with directly. Infrastructure, while essential for enabling that innovation, exists in the background. It’s the scaffolding that holds everything up, not the product that defines the customer experience.\u003c/p\u003e","title":"Infrastructure as an Enabler, Not a differentiator"},{"content":"Rethinking Continuous Delivery Continuous Delivery is the ability to consistently ship code multiple times a day to production. It enables fast and reliable delivery of features through automation of the release process.\nEffective release processes have a few essential qualities:\nSpeed: Changes should be quick to get into production. Simplicity: The process should be easy to understand. Traceability: It should be clear what was included in each release. A powerful approach to achieve these qualities is to center the release process around a build artifact.\nBuild Artifacts A build artifact is the built and packaged output of a codebase. It’s the result of the Continuous Integration (CI) process and represents a specific version or commit of the code. Each artifact is self-contained, allowing for easy inspection and reuse.\nBuild artifacts offer several benefits:\nSecurity: Artifacts can be scanned for supply chain risks or vulnerabilities. Debuggability: Anyone can pull and run the artifact in isolation for troubleshooting. Reproducibility: The artifact’s contents are tied to a specific commit, making it easy to identify the exact code and dependencies used. By using build artifacts as a bridge between Continuous Integration (CI) and Continuous Delivery (CD), CI focuses on creating the artifact, while CD focuses on deploying it. This separation allows for quick and consistent releases, as only the specific artifact and its corresponding configuration need to be updated.\nThis approach is fundamentally different from the process of shipping infrastructure.\nContinuous Delivery of Infrastructure Shipping infrastructure is a different challenge from delivering application code. Infrastructure changes often carry higher operational risks and may require additional safeguards or manual intervention. Unlike application builds, infrastructure changes can be slower to roll out due to the complexity and potential for wide-reaching impact.\nHere’s how infrastructure delivery differs from application delivery:\nRisk: Infrastructure changes may affect uptime, scalability, and security. Speed: Infrastructure changes might take hours to propagate, unlike application changes which can be instant. Process: Infrastructure updates often require approvals, manual checks, or phased rollouts to reduce risk. However, there are scenarios where Continuous Delivery for infrastructure is useful—particularly in Disaster Recovery (DR) processes. In a disaster, the ability to rapidly redeploy infrastructure from scratch can significantly reduce downtime. To achieve this, it’s beneficial to have a separate release process dedicated to infrastructure updates, distinct from the standard application release process.\nWhy Separate Application and Infrastructure Delivery? Keeping the release process for infrastructure separate from the release process for the application code has several advantages:\nFlexibility: Each process can be improved independently, leading to faster iteration and fewer bottlenecks. Risk Reduction: Infrastructure changes often require a higher degree of caution and operational rigor, which may not align with the fast-paced nature of application code delivery. Focus: Teams working on disaster recovery can focus on infrastructure readiness without disrupting the standard code release process. A clear separation also allows for better control over disaster recovery. By isolating the disaster recovery process from the normal deployment process, teams can test recovery scenarios more effectively and ensure that systems can be restored quickly when failures occur.\nConclusion Continuous Delivery isn\u0026rsquo;t a one-size-fits-all approach. While build artifacts are a proven method for streamlining the delivery of application code, infrastructure delivery requires a different strategy. By separating the two, teams can optimize for speed, simplicity, and security in application delivery, while maintaining the operational rigor required for infrastructure changes.\nThis approach allows for faster, more reliable application releases while creating a clear, deliberate process for infrastructure updates and disaster recovery. As teams adopt this separation, they’re better equipped to deliver application changes at speed and handle infrastructure updates with caution and confidence.\n","permalink":"https://lswith.io/posts/continuous-delivery/","summary":"\u003ch1 id=\"rethinking-continuous-delivery\"\u003e\u003cstrong\u003eRethinking Continuous Delivery\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003eContinuous Delivery is the ability to consistently ship code multiple times a day to production. It enables fast and reliable delivery of features through automation of the release process.\u003c/p\u003e\n\u003cp\u003eEffective release processes have a few essential qualities:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSpeed\u003c/strong\u003e: Changes should be quick to get into production.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSimplicity\u003c/strong\u003e: The process should be easy to understand.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTraceability\u003c/strong\u003e: It should be clear what was included in each release.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA powerful approach to achieve these qualities is to center the release process around a build artifact.\u003c/p\u003e","title":"Rethinking Continous Delivery"},{"content":"I attended KubeConf 2018 in Seattle a week ago. I\u0026rsquo;ve decided to distill the core concepts that I think are important from the conference into this blog post.\nI can categorize what the conference was trying to push into 5 concepts:\nObservability is King Do 1 Thing Well Choose Tools Appropriately Everyone Needs to know Cloud Ops Be Cloud Provider Agnostic These aren\u0026rsquo;t new ideas but they were so prominent and spoken about in so many ways that they seemed novel and obvious.\nObservability is King observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs.\nObservability was pushed so much at Kubeconf and was the star of the show. It seemed that all the major tools and new features in Kubernetes were all about increasing visibility into the systems that you run and the systems that you rely on.\nEnvoy and Istio are chosen not only for performance reasons but because people needed a way to observe the network they rely on.\nWorking in the cloud, you quickly realise that not every scenario and failure can be tested for. It\u0026rsquo;s a much better idea to observe what is going on in production and when the alert when things fail.\nPrometheus has now been used extensively in production and has been rolled out in many, many different ways across people\u0026rsquo;s systems. It along with Jaegar seem to be where the industry is moving and consolidating around.\nEven Kubernetes has increased its metric output as it realised that it was required to better maintain the cluster.\nDo 1 Thing Well In an architecture, doing 1 thing well means making sure that each individual part of the system does 1 thing, and has a clear boundary around that 1 thing. This clear boundary means that things become easier to debug, easier to reason about, and easier to compose into something larger.\nIn the tooling space, pick the tool that is easiest to compose with other tools and that only does 1 thing. This allows you to fully leverage it and compose it into your system.\nIn business, doing 1 thing well is about focusing on what is core to your business and outsourcing what isn\u0026rsquo;t. Making sure to do 1 thing well, allows you to be agile enough to change and compose what you provide as necessary and gives focus to the product.\nBeing able to seperate out each part into the 1 thing it does also allows for a better idea of how that thing is being used. This means, that backwards compatibility becomes easier to manage and reliability then increases 10 fold.\nChoose Tools Appropriately Everyone loves tools. Everyone has too many tools. Consolidate into what is necessary, and discard what isn\u0026rsquo;t.\nChoosing the right tools is incredibily important and they should be chosen with care. An acronym that was mentioned was:\nAEIOU:\nApplicable: does this tool solve the issue at hand? Enterprise Readiness: Is this tool ready for scale? Integrate: Can this tool be easily integrated into the platform? Overhead: How much maintainance does this tool require? Useful: Is this tool actually useful? Using this acroynm helps to better evaluate if a tool is right for the job or whether doing something in house is needed.\nEveryone needs to know Cloud Ops To really embrace the cloud, you need a large amount of knowledge in using the cloud. It\u0026rsquo;s synonomous with relearning how to run a server except now its much more complicated.\nBeing able to get everyone trained up to understand how things run in the cloud (or on Kubernetes) means that it\u0026rsquo;s easier for them to reason about how the system interacts. Developers and others need to know how to run and debug their code in any environment.\nThe good thing is that now that Kubernetes is becoming the defactor standard, its becoming much more valuable to learn how things run on Kubernetes and the learnings are much more standardised and universal than ever before.\nBe Cloud Provider Agnostic Kubernetes is a great platform for shipping code anywhere. Your servers, your customer\u0026rsquo;s servers, or someone else\u0026rsquo;s. This also means that writing your code to be cloud agnostic is vital.\nIf you want to truly be able to deploy anywhere, you need to minimise your dependencies on anything that isn\u0026rsquo;t installable on kubernetes.\nThere were a few case studies about how a multitude of products were shipped on their customer\u0026rsquo;s servers through the use of kubernetes.\nHopefully, these concepts will bring the same amount of inspiration that KubeConf has done for me!\n","permalink":"https://lswith.io/posts/kubeconf-2018/","summary":"\u003cp\u003eI attended KubeConf 2018 in Seattle a week ago.\nI\u0026rsquo;ve decided to distill the core concepts that I think are important from the conference into this blog post.\u003c/p\u003e\n\u003cp\u003eI can categorize what the conference was trying to push into 5 concepts:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eObservability is King\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDo 1 Thing Well\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eChoose Tools Appropriately\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEveryone Needs to know Cloud Ops\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBe Cloud Provider Agnostic\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese aren\u0026rsquo;t new ideas but they were so prominent and spoken about in so many ways that they seemed\nnovel and obvious.\u003c/p\u003e","title":"Kubeconf 2018: The Overview"}]